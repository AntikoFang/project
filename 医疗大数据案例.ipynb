{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 CT图像简介\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "# 绘制病人的第70张CT图像\n",
    "plt.imshow(ct_scans[69], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 MHD格式数据读取\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# 数据\n",
    "patient_path = 'CT_MHD_0101.mhd'\n",
    "\n",
    "# 读取数据\n",
    "itk_img = sitk.ReadImage(patient_path)       # ReadImage可以读取MHD格式的数据\n",
    "print(\"图像信息：\", itk_img)\n",
    "\n",
    "# 转换为NumPy数组\n",
    "ct_scans = sitk.GetArrayFromImage(itk_img)   # GetArrayFromImage()可将读取图像信息转为Numpy数组\n",
    "\n",
    "# 获取数组形状\n",
    "print(\"数组的形状：\", ct_scans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 多张CT图像切片可视化\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 画布中的子图以4行4列进行排版\n",
    "fig, plots = plt.subplots(4, 4, figsize=(10, 10))\n",
    "\n",
    "# 使用for循环选取从第一张开始每9张的切片\n",
    "for i in range(0, 139, 9):\n",
    "    \n",
    "    # 根据各子图的索引位置进行绘制\n",
    "    plots[int(i / 36), int((i % 36) / 9)].imshow(ct_scans[i],cmap='gray')\n",
    "\n",
    "    # 隐藏每张子图的坐标轴\n",
    "    plots[int(i / 36), int((i % 36) / 9)].axis('off')\n",
    "    \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4 切片厚度、像素间距和远点坐标的获取\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "\n",
    "# 获取切片厚度及像素间距\n",
    "spacing = np.array(list(reversed(itk_img.GetSpacing())))\n",
    "print(\"CT图像的切片厚度及像素间距是(z, y, x)：\", spacing)\n",
    "\n",
    "#获取图像原点坐标\n",
    "origin = np.array(list(reversed(itk_img.GetOrigin())))\n",
    "print(\"CT图像的原点坐标是(z, y, x)：\", origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.5 图像重采样\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "rescale = [1, 1, 1]\n",
    "\n",
    "ct_scans_shape = ct_scans.shape\n",
    "print(\"原数组形状：\", ct_scans_shape, \" 切片厚度和像素间距：\", spacing)\n",
    "\n",
    "# 计算调整间距后的数组形状\n",
    "resize_factor = spacing / rescale\n",
    "new_real_shape = ct_scans.shape * resize_factor\n",
    "\n",
    "# 四舍五入取整（还是float类型）\n",
    "new_shape = np.around(new_real_shape)\n",
    "\n",
    "# 根据取整后的数组形状求出缩放比例\n",
    "real_resize_factor = new_shape / ct_scans.shape\n",
    "\n",
    "# 得到调整后的间距\n",
    "new_spacing = spacing / real_resize_factor\n",
    "\n",
    "# 使用zoom函数根据比例进行缩放\n",
    "new_ct_scans = ndimage.interpolation.zoom(ct_scans, real_resize_factor, mode=\"nearest\")\n",
    "\n",
    "new_ct_scans_shape = new_ct_scans.shape\n",
    "print(\"新数组形状：\", new_ct_scans_shape, \" 切片厚度和像素间距：\", new_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.7 CT值简介以及3D可视化\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, feature\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "threshold = 300\n",
    "\n",
    "# 转置CT图像数组\n",
    "scans = new_ct_scans.transpose(2,1,0)\n",
    "\n",
    "# 获取等值面\n",
    "verts, faces, norm, val =  measure.marching_cubes_lewiner(scans,threshold,step_size=2)\n",
    "\n",
    "# 创建Figure对象，添加一个新的轴类型为Axes3D\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "    \n",
    "# 三维多边形的集合\n",
    "mesh = Poly3DCollection(verts[faces], alpha=0.7)\n",
    "face_color = [0.45, 0.45, 0.75]\n",
    "mesh.set_facecolor(face_color)\n",
    "\n",
    "# 将mesh添加到坐标轴中\n",
    "ax.add_collection3d(mesh)\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.set_xlim(0, scans.shape[0])\n",
    "ax.set_ylim(0, scans.shape[1])\n",
    "ax.set_zlim(0, scans.shape[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.8 肺实质分割（二值化）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 选取第150张切片\n",
    "scan = new_ct_scans[ 149 ]\n",
    "\n",
    "# 设置阈值生成一个二值图像\n",
    "binary = scan < -320\n",
    "\n",
    "# 绘制两张子图，按照一行两列进行排版\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 绘制切片原图\n",
    "plots[0].imshow(scan, cmap='gray')\n",
    "\n",
    "# 绘制二值化后的图像\n",
    "plots[1].imshow(binary, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.9 肺实质分割（去除边缘区域）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import segmentation \n",
    "\n",
    "# 对上一步二值图像应用skimage.segmentation中清除边界区域的函数\n",
    "cleared = segmentation.clear_border(binary)\n",
    "\n",
    "# 创建子图\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 绘制binary\n",
    "plots[0].imshow(binary,cmap='gray')\n",
    "\n",
    "# 绘制cleared\n",
    "plots[1].imshow(cleared,cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.10 肺实质分割（连通区域分割）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "# 应用skimage.measure中区域标记函数进行连通区域标记\n",
    "label_image = measure.label(cleared)\n",
    "\n",
    "# 创建子图\n",
    "fig, plots = plt.subplots(1,2)\t\n",
    "\n",
    "# 绘制cleared\n",
    "plots[0].imshow(cleared, cmap='gray')\n",
    "\n",
    "# 绘制label_image\n",
    "plots[1].imshow(label_image, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.11 肺实质分割（保留最大肺实质区域）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure\n",
    "\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 首先绘制label_image作为对比\n",
    "plots[0].imshow(label_image, cmap='gray')\n",
    "\n",
    "# 利用regionprops返回每个区域，area属性获每个区域包含的像素个数\n",
    "areas = [r.area for r in measure.regionprops(label_image)]\n",
    "\n",
    "# 对列表排序（升序）\n",
    "areas.sort()\n",
    "\n",
    "# 判断连通区域是否大于两个\n",
    "if len(areas) > 2:\n",
    "    \n",
    "    # 循环每个连通区域\n",
    "    for region in measure.regionprops(label_image):\n",
    "        \n",
    "        # 判断该区域的像素个数是否小于列表的倒数第二个值\n",
    "        if region.area < areas[-2]:\n",
    "            \n",
    "            # coords属性获取区域中每个像素的坐标\n",
    "            for coordinates in region.coords:\n",
    "                \n",
    "                # 根据坐标将该像素值设置为0\n",
    "                label_image[coordinates[0], coordinates[1]] = 0\n",
    "\n",
    "# 重新生成二值图像                \n",
    "new_binary = label_image > 0\n",
    "\n",
    "# 绘制new_binary\n",
    "plots[1].imshow(new_binary, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.12 肺实质分割（腐蚀操作）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology\n",
    "\n",
    "# 生成扁平、圆盘状的结构元素\n",
    "selem = morphology.disk(2)\n",
    "\n",
    "# 对new_binary进行腐蚀操作\n",
    "erosioned = morphology.binary_erosion(new_binary,selem)\n",
    "\n",
    "# 创建子图\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 绘制new_binary\n",
    "plots[0].imshow(new_binary, cmap='gray')\n",
    "\n",
    "# 绘制erosioned\n",
    "plots[1].imshow(erosioned, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.13 肺实质分割（闭合运算）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import morphology\n",
    "\n",
    "# 生成扁平、圆盘状的结构元素\n",
    "selem = morphology.disk(10)\n",
    "\n",
    "# 对erosioned进行闭合操作\n",
    "closed = morphology.binary_closing(erosioned,selem)\n",
    "\n",
    "# 创建子图\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 绘制erosioned\n",
    "plots[0].imshow(erosioned, cmap='gray')\n",
    "\n",
    "# 绘制closed\n",
    "plots[1].imshow(closed, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.14 肺实质分割（孔洞填充）\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "from skimage import filters\n",
    "\n",
    "# 边缘检测，找到肺实质外部轮廓\n",
    "edges = filters.roberts(closed)\n",
    "\n",
    "# 填充肺实质内的孔洞\n",
    "filled = ndimage.binary_fill_holes(edges)\n",
    "\n",
    "# 创建子图\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 绘制closed\n",
    "plots[0].imshow(closed, cmap='gray')\n",
    "\n",
    "# 绘制filled\n",
    "plots[1].imshow(filled, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.15 肺实质分割（掩膜叠加）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取掩膜为零的布尔数组\n",
    "get_high_vals = filled == 0\n",
    "\n",
    "# 原始切片中将掩膜为零的位置设置为-1000\n",
    "scan[get_high_vals] = -1000\n",
    "\n",
    "# 创建子图\n",
    "fig, plots = plt.subplots(1, 2)\n",
    "\n",
    "# 绘制filled\n",
    "plots[0].imshow(filled, cmap='gray')\n",
    "\n",
    "# 绘制scan\n",
    "plots[1].imshow(scan, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.16 肺实质分割（封装代码）\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, morphology, segmentation, filters\n",
    "from scipy import ndimage\n",
    "\n",
    "# 定义肺实质分割函数\n",
    "def get_segmented_lungs(scan, plot=False):\n",
    "   \n",
    "    # 绘制8个步骤的结果图像，以2行4列排版\n",
    "    if plot == True:\n",
    "        fig, plots = plt.subplots(2, 4, figsize=(10, 5))\n",
    "\n",
    "    # 二值化\n",
    "    binary = scan < -320\n",
    "    if plot == True:\n",
    "        plots[0, 0].axis('off')\n",
    "        plots[0, 0].imshow(binary, cmap='gray')\n",
    "    \n",
    "    # 去除边缘区域\n",
    "    cleared = segmentation.clear_border(binary)\n",
    "    if plot == True:\n",
    "        plots[0, 1].axis('off')\n",
    "        plots[0, 1].imshow(cleared, cmap='gray')\n",
    "    \n",
    "    # 连通区域标记\n",
    "    label_image = measure.label(cleared)\n",
    "    if plot == True:\n",
    "        plots[0, 2].axis('off')\n",
    "        plots[0, 2].imshow(label_image, cmap='gray')\n",
    "    \n",
    "    # 保留最大两个肺实质\n",
    "    areas = [r.area for r in measure.regionprops(label_image)]\n",
    "    areas.sort()\n",
    "    if len(areas) > 2:\n",
    "        for region in measure.regionprops(label_image):\n",
    "            if region.area < areas[-2]:\n",
    "                for coordinates in region.coords:\n",
    "                    label_image[coordinates[0], coordinates[1]] = 0\n",
    "    new_binary = label_image > 0\n",
    "    if plot == True:\n",
    "        plots[0, 3].axis('off')\n",
    "        plots[0, 3].imshow(new_binary, cmap='gray')\n",
    "    \n",
    "    # 腐蚀操作\n",
    "    selem = morphology.disk(2)\n",
    "    erosioned = morphology.binary_erosion(new_binary, selem)\n",
    "    if plot == True:\n",
    "        plots[1, 0].axis('off')\n",
    "        plots[1, 0].imshow(erosioned, cmap='gray')\n",
    "    \n",
    "    # 闭合操作\n",
    "    selem = morphology.disk(10)\n",
    "    closed = morphology.binary_closing(erosioned,selem)\n",
    "    if plot == True:\n",
    "        plots[1, 1].axis('off')\n",
    "        plots[1, 1].imshow(closed, cmap='gray')\n",
    "    \n",
    "    # 孔洞填充\n",
    "    edges = filters.roberts(closed)\n",
    "    filled = ndimage.binary_fill_holes(edges)\n",
    "    if plot == True:\n",
    "        plots[1, 2].axis('off')\n",
    "        plots[1, 2].imshow(filled, cmap='gray')\n",
    "    \n",
    "    # 掩膜叠加\n",
    "    get_high_vals = filled == 0\n",
    "    scan[get_high_vals] = -1000\n",
    "    if plot == True:\n",
    "        plots[1, 3].axis('off')\n",
    "        plots[1, 3].imshow(scan, cmap='gray')\n",
    "\n",
    "    return scan\n",
    "\n",
    "# 任意选取一张切片并进行分割展示\n",
    "one_slice = new_ct_scans[ 150 ].copy()\n",
    "get_segmented_lungs(one_slice, True)\n",
    "\n",
    "# 对重采样后的CT图像进行分割\n",
    "segmented = np.asarray([ get_segmented_lungs(slice) for slice in new_ct_scans ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.17 肺实质分割结果3D可视化\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, feature\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "threshold = -1000\n",
    "scans = segmented.transpose(2,1,0)\n",
    "\n",
    "verts, faces, norm, val =  measure.marching_cubes_lewiner(scans,threshold,step_size=2)\n",
    "\n",
    "# 创建Figure对象，添加一个新的轴类型为Axes3D\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "    \n",
    "# 三维多边形的集合\n",
    "mesh = Poly3DCollection(verts[faces], alpha=0.7)\n",
    "face_color = [0.45, 0.45, 0.75]\n",
    "mesh.set_facecolor(face_color)\n",
    "ax.add_collection3d(mesh)\n",
    "\n",
    "# 设置坐标轴\n",
    "ax.set_xlim(0, scans.shape[0])\n",
    "ax.set_ylim(0, scans.shape[1])\n",
    "ax.set_zlim(0, scans.shape[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.19 CT图像坐标系转换\n",
    "import numpy as np\n",
    "\n",
    "# 定义坐标系转换函数，传入参数为待转换世界坐标、原点坐标、各轴间距\n",
    "def world_2_voxel(world_coordinates, origin, spacing):\n",
    "    \n",
    "    # 将世界坐标系转换为图像坐标系\n",
    "    voxel_coordinates = np.absolute(world_coordinates - origin)/spacing\n",
    "    \n",
    "    return voxel_coordinates\n",
    "\n",
    "# 以图像原点坐标为例\n",
    "origin_to_voxel = world_2_voxel(origin,origin,new_spacing)\n",
    "\n",
    "print(\"世界坐标系的原点坐标(z,y,x)：\",origin)\n",
    "print(\"转换为图像坐标系的原点坐标(z,y,x)：\",origin_to_voxel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.20 读取标签文件获取肺结节信息\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 肺结节标签文件路径\n",
    "filename = \"annotations.csv\"\n",
    "\n",
    "# 使用Pandas读取csv文件\n",
    "cands_info = pd.read_csv(filename)\n",
    "\n",
    "# 查看前5行信息\n",
    "print(cands_info.head())\n",
    "\n",
    "# 选取一位病人ID\n",
    "patient_id = \"CT_MHD_0101\"\n",
    "\n",
    "# 从DataFrame中查找该病人的肺结节信息\n",
    "nodule_cands_df = cands_info[cands_info['seriesuid'] == patient_id ]\n",
    "\n",
    "# 生成一个空列表，用于存储肺结节信息\n",
    "nodule_cands = []\n",
    "\n",
    "# 判断病人是否有肺结节\n",
    "if len(nodule_cands_df) > 0:\n",
    "\n",
    "    # 若有肺结节，则依次循环每个结节的信息\n",
    "    for i in range(len(nodule_cands_df)):\n",
    "\n",
    "        # 选取一个肺结节信息保存为Series对象\n",
    "        cand = nodule_cands_df.iloc[i]\n",
    "        \n",
    "        # 获取肺结节中心坐标生成数组，并转换图像坐标\n",
    "        cord_world = np.array([cand[3],cand[2],cand[1]])\n",
    "        cords = world_2_voxel(cord_world,origin,new_spacing)\n",
    "        \n",
    "        # 将坐标与直径添加到列表中\n",
    "        nodule_cands.append([cords, cand[4]])\n",
    "        \n",
    "        # 打印每个肺结节信息\n",
    "        print(\"第{}个肺结节图像坐标（z、y、x）：{}，直径：{}。\".format(i+1,cords, cand[4]))\n",
    "\n",
    "else:\n",
    "    print(patient_id, \"没有肺结节\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.21 肺结节标注及可视化\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 创建全零数组作为初始标签\n",
    "nodule_mask = np.zeros(segmented.shape,dtype=np.int8)\n",
    "\n",
    "# 遍历肺结节信息列表，标注每个肺结节\n",
    "for cand in nodule_cands:\n",
    "    \n",
    "    # 返回不小于真实半径的最小整数\n",
    "    radius = np.ceil(cand[1] / 2)\n",
    "    \n",
    "    # 获取肺结节中心坐标（图像坐标）\n",
    "    core = cand[0]\n",
    "    \n",
    "    # 生成列表用来循环填充区域（到结节中心的距离）\n",
    "    nodule_range = [-radius + i for i in range(int(radius * 2) + 1)]\n",
    "\n",
    "    # 对每个轴依次按nodule_range来循环，遍历立方体中每个像素\n",
    "    for x in nodule_range:\n",
    "        for y in nodule_range:\n",
    "            for z in nodule_range:\n",
    "                \n",
    "                # 得到当前像素坐标\n",
    "                coords = np.array((z,y,x) + core)\n",
    "                \n",
    "                # np.linalg.norm用来计算范数，默认为二范数，即距离\n",
    "                if (np.linalg.norm(np.array((z,y,x)))) < radius:\n",
    "                    \n",
    "                    # 当像素到肺结节中心点坐标小于半径则将其值设置为1\n",
    "                    coords = np.around(coords)\n",
    "                    nodule_mask[int(coords[0]), int(coords[1]), int(coords[2])] = 1\n",
    "\n",
    "# 展示索引为280的切片的原始数据及标签\n",
    "fig, plots = plt.subplots(1, 2, figsize=(10, 10))\n",
    "plots[0].imshow(segmented[280], cmap='gray')\n",
    "plots[1].imshow(nodule_mask[280], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.23 剪裁CT图片\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取不等于背景（-1000）的像素点的各轴索引\n",
    "zz, yy, xx = np.where(segmented != -1000)\n",
    "# 从各轴坐标中求出最小和最大索引，此时索引组成的立方体即为裁剪区域\n",
    "box = np.array([\n",
    "    [np.min(zz), np.max(zz)], \n",
    "    [np.min(yy), np.max(yy)],\n",
    "    [np.min(xx), np.max(xx)]])\n",
    "\n",
    "# 将立方体扩展，每个轴两边增加5个像素单位\n",
    "extendbox = np.vstack([np.max([[0, 0, 0], box[:, 0] - 5], axis=0),\n",
    "                    np.min([segmented.shape, box[:,1] + 5], axis=0)]\n",
    "                     ).T\n",
    "\n",
    "# 根据索引裁剪图像\n",
    "new_lung_mask = segmented[extendbox[0, 0]:extendbox[0, 1],\n",
    "                        extendbox[1, 0]:extendbox[1, 1],\n",
    "                        extendbox[2, 0]:extendbox[2, 1]]\n",
    "\n",
    "new_nodule_mask = nodule_mask[extendbox[0, 0]:extendbox[0, 1],\n",
    "                        extendbox[1, 0]:extendbox[1, 1],\n",
    "                        extendbox[2, 0]:extendbox[2, 1]]\n",
    "\n",
    "# 查看裁剪后CT图像的一张切片\n",
    "fig = plt.figure()\n",
    "plt.imshow(new_lung_mask[150], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.24 缩放及扩充CT图像\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "# 按0.7 的比例缩放图像\n",
    "new_lung_mask = ndimage.zoom(new_lung_mask,(0.7,0.7,0.7), order=0)\n",
    "new_nodule_mask = ndimage.zoom(new_nodule_mask,(0.7,0.7,0.7), order=0)\n",
    "  \n",
    "# 计算缩放后图像各轴长度与256的差值\n",
    "y_pad = 256 - new_lung_mask.shape[1]\n",
    "x_pad = 256 - new_lung_mask.shape[2]\n",
    "    \n",
    "# 计算y、x轴两边需填充的长度\n",
    "lower_ypad = y_pad//2\n",
    "upper_ypad = y_pad - lower_ypad\n",
    "lower_xpad = x_pad//2\n",
    "upper_xpad = x_pad - lower_xpad\n",
    "\n",
    "# 填充肺实质图像\n",
    "lung_mask_256 = np.pad(new_lung_mask,\n",
    "                      ((0,0),(lower_ypad,upper_ypad),\n",
    "                      (lower_xpad,upper_xpad)),\n",
    "                      mode='constant',constant_values=-1000)\n",
    "\n",
    "# 填充肺结节标签数组\n",
    "nodule_mask_256 = np.pad(new_nodule_mask,\n",
    "                      ((0,0),(lower_ypad,upper_ypad),\n",
    "                      (lower_xpad,upper_xpad)),\n",
    "                      mode='constant',constant_values=0)\n",
    "\n",
    "    \n",
    "# 打印最终数组形状\n",
    "print(\"肺实质图像数组的形状：\", lung_mask_256.shape)\n",
    "print(\"肺结节标签数组的形状：\", nodule_mask_256.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.25 图像像素值标准化\n",
    "import numpy as np\n",
    "\n",
    "# 设置边界\n",
    "MIN_BOUND = -1000.0\n",
    "MAX_BOUND = 400.0\n",
    "\n",
    "# 对图像标准化\n",
    "lung_mask_256 = (lung_mask_256 - MIN_BOUND) / (MAX_BOUND - MIN_BOUND) * 255\n",
    "\n",
    "# 将大于255 的值设置为255\n",
    "lung_mask_256[lung_mask_256 > 255] = 255.\n",
    "\n",
    "# 将小于0 的值设置为0\n",
    "lung_mask_256[lung_mask_256 < 0] = 0.\n",
    "\n",
    "# 最后设置数据类型\n",
    "lung_mask_256 = lung_mask_256.astype(np.float16)\n",
    "\n",
    "print(\"最大值：\", lung_mask_256.max())\n",
    "print(\"最小值：\", lung_mask_256.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.26 肺结节切片的提取及保存\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 通过对标签切片求和来筛选出包含肺结节的切片索引\n",
    "included_slices = [z for z in range(nodule_mask_256.shape[0]) if np.sum(nodule_mask_256[z]) > 0]\n",
    "\n",
    "# 按照筛选出的索引获取这些切片\n",
    "lung_mask_256 = lung_mask_256[included_slices]\n",
    "nodule_mask_256 = nodule_mask_256[included_slices]\n",
    "\n",
    "# 展示一组图片\n",
    "show_slice = int(lung_mask_256.shape[0] / 2)\n",
    "\n",
    "fig, plots = plt.subplots(1, 2, figsize=(10, 10))\n",
    "plots[0].imshow(lung_mask_256[show_slice].astype(np.int16), cmap='gray')\n",
    "plots[1].imshow(nodule_mask_256[show_slice], cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 数据读取\n",
    "import numpy as np\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 数据所在的文件夹\n",
    "file_path = './userdcm/'\n",
    "one_dcm = '000089.dcm'\n",
    "\n",
    "# 利用dcmread()读取.dcm文件\n",
    "one_slice = pydicom.dcmread(file_path + one_dcm)\n",
    "    \n",
    "# 打印像素间距\n",
    "print(\"像素间距：\", one_slice.PixelSpacing)\n",
    "\n",
    "# 打印切片左上的角世界坐标\n",
    "print(\"切片左上角的世界坐标：\", one_slice.ImagePositionPatient)\n",
    "\n",
    "# 获取像素值\n",
    "one_pixel_array = one_slice.pixel_array\n",
    "\n",
    "# 可视化切片\n",
    "fig = plt.figure()\n",
    "plt.imshow(one_pixel_array, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# 利用dcmread()函数读取每张切片文件并生成一个列表\n",
    "slices = [pydicom.dcmread(file_path + s) for s in os.listdir(file_path)]\n",
    "\n",
    "# 按照每个切片左上角世界坐标的z轴排序\n",
    "slices.sort(key = lambda x: x.ImagePositionPatient[2])\n",
    "\n",
    "print(\"该CT图像共{}张切片。\".format(len(slices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4 像素值转换CT值\n",
    "import numpy as np\n",
    "import pydicom\n",
    "\n",
    "print(\"斜率：\", one_slice.RescaleSlope)\n",
    "\n",
    "print(\"截距：\", one_slice.RescaleIntercept)\n",
    "\n",
    "# 获取每张切片的数组值并堆叠为一个Numpy数组\n",
    "ct_scans = np.stack([s.pixel_array for s in slices])\n",
    "\n",
    "# 转换前\n",
    "pixel_max = ct_scans.max()\n",
    "pixel_min = ct_scans.min()\n",
    "print(\"转换前像素值最大值：{}，最小值：{}\".format(pixel_max, pixel_min))\n",
    "\n",
    "# 对每张切片应用线性变换转CT值\n",
    "for slice_number in range(len(slices)):\n",
    "    \n",
    "    # 获取截距和斜率\n",
    "    intercept = slices[slice_number].RescaleIntercept\n",
    "    slope = slices[slice_number].RescaleSlope\n",
    "    \n",
    "    # 线性转换为CT值\n",
    "    ct_scans[slice_number] = slope * ct_scans[slice_number] + np.int16(intercept)\n",
    "\n",
    "# 转换后\n",
    "ct_max = ct_scans.max()\n",
    "ct_min = ct_scans.min()\n",
    "print(\"转换后CT值最大值：{}，最小值：{}\".format(ct_max, ct_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5 CT图像重采样\n",
    "from scipy import ndimage\n",
    "import numpy as np\n",
    "\n",
    "rescale = [1, 1, 1]\n",
    "\n",
    "ct_scans_shape = ct_scans.shape\n",
    "print(\"原数组形状：\", ct_scans_shape, \" 切片厚度和像素间距：\", spacing)\n",
    "\n",
    "# 计算调整间距后的数组形状\n",
    "resize_factor = spacing / rescale\n",
    "new_real_shape = ct_scans.shape * resize_factor\n",
    "\n",
    "# 四舍五入取整（还是float类型）\n",
    "new_shape = np.around(new_real_shape)\n",
    "\n",
    "# 根据取整后的数组形状求出缩放比例\n",
    "real_resize_factor = new_shape / ct_scans.shape\n",
    "\n",
    "# 得到调整后的间距\n",
    "new_spacing = spacing / real_resize_factor\n",
    "\n",
    "# 使用zoom函数根据比例进行缩放\n",
    "new_ct_scans = ndimage.interpolation.zoom(ct_scans,real_resize_factor,mode='nearest')\n",
    "\n",
    "new_ct_scans_shape = new_ct_scans.shape\n",
    "print(\"新数组形状：\", new_ct_scans.shape, \" 切片厚度和像素间距：\", new_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.6 肺实质分割及统一图像尺寸\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import measure, morphology, segmentation, filters\n",
    "from scipy import ndimage\n",
    "\n",
    "# 调用get_segmented_lungs函数对CT图像进行肺实质的分割\n",
    "segmented = np.array([get_segmented_lungs(slice) for slice in new_ct_scans])\n",
    "\n",
    "# 获取不等于背景（-1000）的像素点的各轴索引\n",
    "zz, yy, xx = np.where(segmented != -1000)\n",
    "\n",
    "# 从各轴坐标中求出最小和最大索引，此时索引组成的立方体即为裁剪区域\n",
    "box = np.array([\n",
    "    [np.min(zz), np.max(zz)], \n",
    "    [np.min(yy), np.max(yy)],\n",
    "    [np.min(xx), np.max(xx)]])\n",
    "\n",
    "# 将立方体扩展，每个轴两边增加5\n",
    "extendbox = np.vstack([np.max([[0, 0, 0], box[:, 0] - 5], axis=0),\n",
    "                np.min([segmented.shape, box[:, 1] + 5], axis=0)]\n",
    "                ).T\n",
    "\n",
    "# 根据索引裁剪图像\n",
    "new_lung_mask = segmented[extendbox[0, 0]:extendbox[0, 1],\n",
    "                        extendbox[1, 0]:extendbox[1, 1],\n",
    "                        extendbox[2, 0]:extendbox[2, 1]]\n",
    "\n",
    "# 按0.7 的比例缩放图像\n",
    "new_lung_mask = ndimage.zoom(new_lung_mask,(0.7,0.7,0.7), order=0)\n",
    "\n",
    "# 计算当前尺寸与256的差值\n",
    "y_pad = 256 - new_lung_mask.shape[1]\n",
    "x_pad = 256 - new_lung_mask.shape[2]\n",
    "\n",
    "# 计算两边填充长度\n",
    "lower_ypad = y_pad // 2\n",
    "upper_ypad = y_pad - lower_ypad\n",
    "lower_xpad = x_pad // 2\n",
    "upper_xpad = x_pad - lower_xpad\n",
    "\n",
    "# y，x轴按照指定长度填充背景值\n",
    "lung_mask_256 = np.pad(new_lung_mask,\n",
    "                      ((0,0),(lower_ypad,upper_ypad),\n",
    "                      (lower_xpad,upper_xpad)),\n",
    "                      mode='constant',constant_values=-1000)\n",
    "\n",
    "# 打印最终数组形状\n",
    "lung_mask_256_shape = lung_mask_256.shape\n",
    "print(\"统一图像尺寸后数组形状：\", lung_mask_256_shape)\n",
    "\n",
    "# 选取一张展示\n",
    "fig = plt.figure()\n",
    "plt.imshow(lung_mask_256[149], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.7 数据标准化及保存\n",
    "import numpy as np\n",
    "\n",
    "# 设置边界\n",
    "MIN_BOUND = -1000.0\n",
    "MAX_BOUND = 400.0\n",
    "\n",
    "# 对图像标准化\n",
    "lung_mask_256 = (lung_mask_256 - MIN_BOUND) / (MAX_BOUND - MIN_BOUND) * 255\n",
    "\n",
    "# 将大于255 的值设置为255\n",
    "lung_mask_256[lung_mask_256 > 255] = 255\n",
    "\n",
    "# 将小于0 的值设置为0\n",
    "lung_mask_256[lung_mask_256 < 0] = 0\n",
    "\n",
    "# 最后设置数据类型\n",
    "lung_mask_256 = lung_mask_256.astype(np.float16)\n",
    "\n",
    "print(\"最大值：\", lung_mask_256.max())\n",
    "print(\"最小值：\", lung_mask_256.min()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第六章 U-Net肺结节检测模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Tensorflow中的卷积层和激活函数\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input_exp = tf.constant(2., shape=[1, 32, 32, 1])\n",
    "\n",
    "# 创建卷积核\n",
    "filters = tf.Variable(tf.constant(1., shape=[3, 3, 1, 16]), name='kernel')\n",
    "\n",
    "# 创建偏置项\n",
    "bias = tf.Variable(tf.constant(0.,shape=[16]), name='bias')\n",
    "\n",
    "# 进行卷积操作\n",
    "conv = tf.nn.conv2d(input_exp, filters, strides=[1,1,1,1], padding='SAME')\n",
    "print(conv)\n",
    "\n",
    "# 添加偏置\n",
    "conv_b = tf.nn.bias_add(conv,bias)\n",
    "print(conv_b)\n",
    "\n",
    "# 应用激活函数\n",
    "conv_relu = tf.nn.relu(conv_b)\n",
    "\n",
    "print(conv_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 定义创建卷积层的函数\n",
    "import tensorflow as tf\n",
    "\n",
    "# 创建卷积层\n",
    "def conv2d(x, filter_size, input_channels, filters, with_activation=True):\n",
    "    with tf.name_scope(\"conv2d\"):\n",
    "        \n",
    "        # 根据传入参数确定卷积核的形状\n",
    "        filter_shape = [filter_size, filter_size, input_channels, filters]\n",
    "        \n",
    "        # 创建卷积核和偏置项\n",
    "        initializer=tf.contrib.layers.xavier_initializer()        \n",
    "        w = tf.Variable(initializer(filter_shape), name=\"kernel\")\n",
    "        b = tf.Variable(tf.truncated_normal([filters],stddev=0.1),name=\"bias\")\n",
    "        \n",
    "        # 创建卷积操作\n",
    "        conv = tf.nn.conv2d(x,w,strides=[1,1,1,1],padding='SAME')\n",
    "        \n",
    "        # 添加偏置\n",
    "        conv_b = tf.nn.bias_add(conv, b)\n",
    "        \n",
    "        # 是否应用激活函数\n",
    "        if with_activation:\n",
    "            \n",
    "            # 返回应用激活函数后的输出\n",
    "            return tf.nn.relu(conv_b)\n",
    "        \n",
    "        return conv_b\n",
    "\n",
    "# 示例\n",
    "input_data = tf.constant(0.1, shape=[1, 4, 4, 1])\n",
    "\n",
    "conv1 = conv2d(input_data, 3, 1, 8)\n",
    "print(conv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.4 收缩路径网络结构搭建\n",
    "import tensorflow as tf\n",
    "\n",
    "# 创建占位符\n",
    "data = tf.placeholder(tf.float32, shape=[None, 256, 256])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 添加channel， 即为数据加一个维度。\n",
    "input_data = tf.expand_dims(data, axis=-1)\n",
    "\n",
    "# 构建网络收缩路径结构\n",
    "# down_conv_blok1\n",
    "conv1a = conv2d(input_data, 3, 1, 32)      # 输出：? * 256 * 256 * 32\n",
    "conv1a = tf.nn.dropout(conv1a, keep_prob)   \n",
    "conv1b = conv2d(conv1a, 3, 32, 32)         # 输出：? * 256 * 256 * 32\n",
    "maxpool1 = tf.nn.max_pool(conv1b, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')             # 输出：? * 128 * 128 * 32\n",
    "\n",
    "# down_conv_blok2\n",
    "conv2a = conv2d(maxpool1, 3, 32, 80)       # 输出：? * 128 * 128 * 80\n",
    "conv2a = tf.nn.dropout(conv2a, keep_prob)\n",
    "conv2b = conv2d(conv2a, 3, 80, 80)         # 输出：? * 128 * 128 * 80\n",
    "maxpool2 = tf.nn.max_pool(conv2b, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')             # 输出：? * 64 * 64 * 80\n",
    "\n",
    "# down_conv_blok3\n",
    "conv3a = conv2d(maxpool2, 3, 80, 160)      # 输出：? * 64 * 64 * 160\n",
    "conv3a = tf.nn.dropout(conv3a, keep_prob)\n",
    "conv3b = conv2d(conv3a,3, 160, 160)       # 输出：? * 64 * 64 * 160\n",
    "maxpool3 = tf.nn.max_pool(conv3b, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')            # 输出：? * 32 * 32 * 160\n",
    "\n",
    "# 中间卷积层\n",
    "conv4a = conv2d(maxpool3, 3, 160, 320)     # 输出：? * 32 * 32 * 320 \n",
    "conv4a = tf.nn.dropout(conv4a, keep_prob)\n",
    "conv4b = conv2d(conv4a, 3, 320, 320)       # 输出：? * 32 * 32 * 320\n",
    "\n",
    "print(conv4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.5 拓展路径网络结构搭建\n",
    "import tensorflow as tf\n",
    "\n",
    "# 构建网络扩展路径结构\n",
    "# up_conv_blok1\n",
    "up_conv1 = tf.image.resize_images(conv4b, [64, 64])    # 输出：? * 64 * 64 * 320 \n",
    "concat1 = tf.concat([up_conv1, conv3b], axis=-1)    # 输出：? * 64 * 64 * 480\n",
    "conv5a = conv2d(concat1, 3, 480, 160)    # 输出：? * 64 * 64 * 160\n",
    "conv5a = tf.nn.dropout(conv5a, keep_prob)\n",
    "conv5b = conv2d(conv5a, 3, 160, 160)    # 输出：? * 64 * 64 * 160\n",
    "\n",
    "# up_conv_blok2\n",
    "up_conv2 = tf.image.resize_images(conv5b, [128, 128])    # 输出：? * 128 * 128 * 160\n",
    "concat2 = tf.concat([up_conv2,conv2b], axis=-1)                                 # 输出：? * 128 * 128 * 240\n",
    "conv6a = conv2d(concat2, 3, 240, 80)    # 输出：? * 128 * 128 * 80\n",
    "conv6a = tf.nn.dropout(conv6a, keep_prob)\n",
    "conv6b = conv2d(conv6a, 3, 80, 80)    # 输出：? * 128 * 128 * 80\n",
    "\n",
    "# up_conv_blok3\n",
    "up_conv3 = tf.image.resize_images(conv6b, [256, 256])    # 输出：? * 256 * 256 * 80\n",
    "concat3 = tf.concat([up_conv3, conv1b], axis=-1)    # 输出：? * 256 * 256 * 112\n",
    "conv7a = conv2d(concat3, 3, 112, 32)                  # 输出：? * 256 * 256 * 32\n",
    "conv7a = tf.nn.dropout(conv7a, keep_prob)\n",
    "conv7b = conv2d(conv7a, 3, 32, 32)    # 输出：? * 256 * 256 * 32\n",
    "\n",
    "# 输出层\n",
    "predictions = conv2d(conv7b, 3, 32, 2, False)    # 输出：? * 256 * 256 * 2\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.6 损失函数、优化器及模型持久化\n",
    "import tensorflow as tf\n",
    "\n",
    "# 标签数据占位符\n",
    "labels = tf.placeholder(tf.int32,shape=[None, 256, 256])\n",
    "\n",
    "# 设置权重\n",
    "weights = (200 - 1) * tf.to_float(labels) + 1\n",
    "\n",
    "# 设置交叉熵损失函数\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "        labels=labels, logits=predictions, weights=weights)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "\n",
    "# 模型的保存和恢复变量\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.7 定义验证函数\n",
    "import tensorflow as tf\n",
    "\n",
    "def validate(session, x, y, batch_size):\n",
    "    \n",
    "    # 用于存放每个batch的损失\n",
    "    losses = []\n",
    "    \n",
    "    # 循环每个批次\n",
    "    for start in range(0, len(x), batch_size):\n",
    "        \n",
    "        # 将每个batch数据传入占位符\n",
    "        feed_dict = {}\n",
    "        feed_dict[data] = x[start: start + batch_size]\n",
    "        feed_dict[labels] = y[start: start + batch_size]\n",
    "        \n",
    "        # 在验证时不应该进行dropout，所以keep_prob设置为1\n",
    "        feed_dict[keep_prob] = 1.\n",
    "        \n",
    "        # 目标运算\n",
    "        fetches = loss\n",
    "        \n",
    "        # 调用session.run()计算每个batch的损失\n",
    "        outputs = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # 每个batch的loss加入列表\n",
    "        losses.append(outputs)\n",
    "\n",
    "    # 返回整个传入数据集的平均损失\n",
    "    return sum(losses) / float(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.8 定义训练函数\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "def train(session, x_train, y_train, x_val, y_val, epochs, batch_size, train_dir, best_loss):\n",
    "\n",
    "    # 根据回合数进行循环\n",
    "    for e in range(epochs):\n",
    "        print(\"[Epoch %s of %s]\" % (e + 1, epochs))\n",
    "\n",
    "        # 创建训练数据的索引并随机打乱\n",
    "        indices = np.random.permutation(y_train.shape[0])\n",
    "        \n",
    "        # start为每个batch的其实索引，循环的步长为batch_size\n",
    "        for start in range(0, len(x_train), batch_size):\n",
    "            \n",
    "            # 将一个batch数据传入占位符\n",
    "            feed_dict = {}\n",
    "            feed_dict[data] = x_train[indices[start: start + batch_size]]\n",
    "            feed_dict[labels] = y_train[indices[start: start + batch_size]]\n",
    "            \n",
    "            # 在训练时要应用dropout，这里设置keep_prob为0.8\n",
    "            feed_dict[keep_prob] = 0.8\n",
    "            \n",
    "            # 目标运算\n",
    "            fetches = optimizer\n",
    "            \n",
    "            # 运行计算\n",
    "            session.run(fetches, feed_dict)\n",
    "        \n",
    "        # 获取训练集上的损失\n",
    "        train_loss = validate(session, x_train, y_train, batch_size)\n",
    "        \n",
    "        # 获取验证集上的损失\n",
    "        val_loss = validate(session, x_val, y_val, batch_size)\n",
    "        print(\"训练集损失：%s；验证集损失：%s\" % (train_loss, val_loss))\n",
    "        \n",
    "        # 如果损失达到了设定的损失值，则将当前模型参数保存。\n",
    "        if val_loss <= best_loss:\n",
    "            print(\"新的最小验证集损失：%s，保存模型!\" % (val_loss))\n",
    "            \n",
    "            # 将当前最小损失赋值给best_val_loss\n",
    "            best_loss = val_loss\n",
    "\n",
    "            # 保存模型\n",
    "            saver.save(session, train_dir + 'model_unet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.9 数据集的读取与划分\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置数据路径\n",
    "data_dir = './data_dir/'\n",
    "labels_dir = './labels_dir/'\n",
    "\n",
    "# 获取路径中的文件列表\n",
    "data_files = os.listdir(data_dir)\n",
    "data_files.sort()\n",
    "\n",
    "# 创建保存数据的列表\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# 遍历文件列表中的每个文件\n",
    "for num in range(len(data_files)):\n",
    "    \n",
    "    # 获取文件路径\n",
    "    patient_id = data_files[num][:-14]\n",
    "    data_path = data_dir + patient_id + '_lung_mask.npy'\n",
    "    label_path = labels_dir + patient_id + '_nodule_mask.npy'\n",
    "    \n",
    "    # 加载文件并添加列表中\n",
    "    x.append(np.load(data_path))\n",
    "    y.append(np.load(label_path))\n",
    "\n",
    "print(\"数据量：%s 个病人\" %  len(x))\n",
    "\n",
    "# 利用np.concatenate将列表内的数组拼接为一个数组\n",
    "x = np.concatenate(x).astype(np.float32)\n",
    "y = np.concatenate(y).astype(np.int8)\n",
    "\n",
    "print(\"总切片数：\", x.shape[0])\n",
    "\n",
    "# 划分训练集和验证集\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size = 0.3,random_state=1024)\n",
    "\n",
    "print(\"训练集：{}；验证集：{}\".format(x_train.shape[0], x_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.10 模型训练\n",
    "import tensorflow as tf\n",
    "\n",
    "# 设置模型保存路径\n",
    "train_dir = './train_dir/'\n",
    "\n",
    "# 在上下文语境中创建TensorFlow会话\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.exists(train_dir):\n",
    "               \n",
    "        # 加载模型参数\n",
    "        saver.restore(sess, train_dir + \"model_unet_loss\")\n",
    "    else:\n",
    "        print(\"创建新的模型！\")\n",
    "        \n",
    "        # 初始化所有参数\n",
    "        tf.global_variables_initializer()\n",
    "        \n",
    "    # 获取所有可训练变量并打印\n",
    "    parameter_num = sum(v.shape.num_elements() for v in tf.trainable_variables())\n",
    "    print('模型可训练参数总量：%d' % parameter_num)\n",
    "        \n",
    "    # 调用train函数设置超参数进行训练\n",
    "    train(sess, x_train, y_train, x_val, y_val, epochs=5, batch_size=4, train_dir=train_dir, best_loss=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第七章 肺结节检测与筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 定义检测函数\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 应用softmax\n",
    "predict_softmax = tf.nn.softmax(predictions)\n",
    "\n",
    "def predict(session, x, batch_size):\n",
    "    \n",
    "    # 创建全0标签\n",
    "    result = np.zeros((x.shape[0], 256, 256))\n",
    "    \n",
    "    # 按批次进行计算\n",
    "    for start in range(0, len(x), batch_size):\n",
    "        \n",
    "        # 将每个batch数据传入占位符\n",
    "        feed_dict = {}\n",
    "        feed_dict[data] = x[start: start + batch_size]\n",
    "        \n",
    "        # 在检测时不应该进行dropout，所以keep_prob设置为1\n",
    "        feed_dict[keep_prob] = 1\n",
    "\n",
    "        fetches = predict_softmax\n",
    "        \n",
    "        # 获取神经网络的输出\n",
    "        outputs = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # 获取类别为1的概率\n",
    "        result[start:start + batch_size] = outputs[:,:,:,1]\n",
    "\n",
    "    return result.astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 数据读取与肺结节检测\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "train_dir = './train_dir/'\n",
    "predict_data_dir = './predict_data_dir/'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        \n",
    "    # 加载模型\n",
    "    saver.restore ( sess, train_dir + \"model_unet_loss\")\n",
    "    \n",
    "    data_list = os.listdir(predict_data_dir)\n",
    "    data_list.sort()\n",
    "    print(\"数据量：%s 个病人\" %  len(data_list))\n",
    "    \n",
    "    # 遍历每个病人数据应用模型\n",
    "    for num in range(len(data_list)):\n",
    "        \n",
    "        # 加载数据\n",
    "        path = predict_data_dir + data_list[num]\n",
    "        x = np.load(path)\n",
    "        \n",
    "        patient_id = data_list[num][:-14]\n",
    "        print(\"正在检测：%s\" % patient_id)\n",
    "        \n",
    "        # 调用predicat进行肺结节检测\n",
    "        result = predicat(sess, x, 8)\n",
    "\n",
    "        print(\"成功检测：%s\" % patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.4 检测结果可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 加载数据\n",
    "data_one = np.load(\"CT_DICOM_0009_lung_mask.npy\").astype(np.int16)\n",
    "result_one = np.load(\"CT_DICOM_0009.npy\")\n",
    "\n",
    "# 设置展示的切片索引\n",
    "show_slice = 100\n",
    "\n",
    "# 绘图展示\n",
    "fig, plots = plt.subplots(1, 2, figsize=(10,10))\n",
    "plots[0].imshow(data_one[show_slice], cmap='gray')\n",
    "\n",
    "# 网络输出的像素值是该像素是肺结节的概率，因此以0.5为阈值转换为0和1。\n",
    "plots[1].imshow(result_one[show_slice] > 0.5, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.6 初始化候选块\n",
    "import numpy as np\n",
    "\n",
    "def init_candidate_cubes(result_label):\n",
    "    \n",
    "    # 创建候选块存放列表\n",
    "    candidate_cubes = []\n",
    "    \n",
    "    # 初始化四个候选块为CT图像的角\n",
    "    init_cube1 = result_label[:32, :32, :32]\n",
    "    init_cube2 = result_label[:32, 224:, :32]\n",
    "    init_cube3 = result_label[:32, 224:, 224:]\n",
    "    init_cube4 = result_label[:32, :32, 224:]\n",
    "    \n",
    "    # 将四个候选块添加到列表中\n",
    "    candidate_cubes.append((np.sum(init_cube1), 0, 0, 0))\n",
    "    candidate_cubes.append((np.sum(init_cube2), 0, 224, 0))\n",
    "    candidate_cubes.append((np.sum(init_cube3), 0, 224, 224))\n",
    "    candidate_cubes.append((np.sum(init_cube4), 0, 0, 224))\n",
    "    \n",
    "    return candidate_cubes\n",
    "\n",
    "# 示例\n",
    "result_label = np.load('result_label_0001.npy')\n",
    "candidate_cubes = init_candidate_cubes(result_label)\n",
    "for i in range(len(candidate_cubes)):\n",
    "    print(\"初始候选块{}：{}\".format(i + 1, candidate_cubes[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.7 候选块的更新规则\n",
    "import numpy as np\n",
    "\n",
    "def updata_candidate_cube(candidate_cubes, test_score, test_location):\n",
    "    num_overlapping = 0\n",
    "    overlapping_cube = None\n",
    "    \n",
    "    # 判断当前新候选块与列表中的候选块是否有交集\n",
    "    for cube in candidate_cubes:\n",
    "        cube_location = np.array(cube[1:])\n",
    "        \n",
    "        # 当所有轴的坐标之差都小于等于32，则存在交集\n",
    "        if np.all(cube_location - test_location <= 32):\n",
    "            num_overlapping += 1\n",
    "            if num_overlapping > 1:\n",
    "                return  \n",
    "            overlapping_cube = cube\n",
    "            \n",
    "    new_cube = (test_score, test_location[0], test_location[1], test_location[2])\n",
    "    \n",
    "    # 如果都没有交集则直接去掉列表中概率最小的候选块 并将当前候选块添加进去\n",
    "    if num_overlapping == 0: \n",
    "        candidate_cubes.remove(min(candidate_cubes, key = lambda t : t[0]))\n",
    "        candidate_cubes.append(new_cube)\n",
    "    \n",
    "    # 如果有一个有交集的\n",
    "    if num_overlapping == 1: \n",
    "        \n",
    "        # 并且新的立方体概率值更大，则移除与之存在交集的立方体插入新的候选块\n",
    "        if overlapping_cube[0] < test_score:\n",
    "            \n",
    "            candidate_cubes.remove(overlapping_cube)\n",
    "            candidate_cubes.append(new_cube)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.8 筛选候选块\n",
    "import numpy as np\n",
    "\n",
    "def find_candidate_cubes(result_label):  \n",
    "    \n",
    "    # 防止数据不稳定\n",
    "    result_label /= np.amax(result_label) \n",
    "    \n",
    "    # 初始化候选块\n",
    "    candidate_cubes = init_candidate_cubes(result_label)\n",
    "    \n",
    "    # 初始化最小概率值为0\n",
    "    min_score = 0\n",
    "    \n",
    "    # 遍历标签数据，y、x轴步长为4\n",
    "    for i in range(result_label.shape[0] - 31):\n",
    "        for j in range(0, 256 - 31, 4):\n",
    "            for k in range(0, 256 - 31, 4):\n",
    "                \n",
    "                # 获取当前立方体\n",
    "                test_cube = result_label[i: i + 32, j: j + 32, k: k + 32]\n",
    "                \n",
    "                # 计算概率（求和）\n",
    "                test_score = np.sum(test_cube)\n",
    "                \n",
    "                # 如果大于当前候选块中最小的概率值则更新候选块\n",
    "                if(test_score > min_score):\n",
    "                    updata_candidate_cube(candidate_cubes, test_score, np.array((i, j, k)))\n",
    "                    \n",
    "                    # 重新设置当前概率的最小值\n",
    "                    min_score = min(candidate_cubes, key = lambda t : t[0])[0]\n",
    "    \n",
    "    # 按照概率进行排序                \n",
    "    sorted_candidate_cubes = sorted(candidate_cubes, key=lambda tup : tup[0], reverse=True)\n",
    "\n",
    "    return sorted_candidate_cubes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.9 候选块的筛选与拼接\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "  \n",
    "# 加载原始图像与标签数据,数据类型转换为float32\n",
    "result_label = np.load('./unet_result_dir/CT_DICOM_0009.npy').astype(np.float32)\n",
    "origin_data = np.load('./predict_data_dir/CT_DICOM_0009_lung_mask.npy').astype(np.float32)\n",
    "\n",
    "# 调用函数获取候选块列表\n",
    "candidate_cubes = find_candidate_cubes(result_label)\n",
    "\n",
    "# 获取原图像\n",
    "origin_data_cubes = []\n",
    "for i in range(4):\n",
    "    cube = candidate_cubes[i]\n",
    "\n",
    "    # 根据候选块的坐标位置获取原图像\n",
    "    origin_data_cubes.append(origin_data[cube[1]:cube[1] + 32,\n",
    "                       cube[2]:cube[2] + 32, cube[3]:cube[3] + 32])\n",
    "\n",
    "# 对原图像的立方体块进行拼接\n",
    "combined_cubes_top = np.concatenate(\n",
    "    [origin_data_cubes[0], origin_data_cubes[1]], axis=2) \n",
    "combined_cubes_bottom = np.concatenate(\n",
    "\t [origin_data_cubes[2], origin_data_cubes[3]], axis=2)\n",
    "combined_cubes = np.concatenate(\n",
    "    [combined_cubes_top, combined_cubes_bottom], axis=1)\n",
    "\n",
    "# 绘制拼接结果的单张切片\n",
    "plt.imshow(combined_cubes[10],cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第八章 肺癌诊断神经网络模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.2 定义创建网络结构的函数\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 创建权重函数\n",
    "def weight_variable(shape, name=\"kernel\"):\n",
    "    \n",
    "    # 创建初始化器\n",
    "    initializer = tf.contrib.layers.xavier_initializer()\n",
    "    return tf.Variable(initializer(shape), name=name)\n",
    "\n",
    "# 创建偏置函数\n",
    "def bias_variable(shape, name=\"bias\"):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "# 3D卷积\n",
    "def conv3d_with_bn(x, filter_size, input_channels, filters, is_training):\n",
    "    with tf.name_scope(\"conv3d\"):\n",
    "        shape = [filter_size, filter_size, filter_size, input_channels, filters]\n",
    "        w = weight_variable(shape)\n",
    "        b = bias_variable([filters])\n",
    "        \n",
    "        # 利用tf.nn.conv3d进行3D卷积操作\n",
    "        conv_3d = tf.nn.conv3d(x, w, strides=[1, 1, 1, 1, 1], padding='SAME')\n",
    "        conv_3d_b = tf.nn.bias_add(conv_3d, b)\n",
    "        relu = tf.nn.relu(conv_3d_b)\n",
    "        \n",
    "        # 应用batch_normalization\n",
    "        bn = tf.layers.batch_normalization(relu, training=is_training)\n",
    "        return bn\n",
    "\n",
    "# 全连接\n",
    "def new_fc(x, num_inputs, num_outputs):\n",
    "    w = weight_variable([num_inputs, num_outputs])\n",
    "    b = bias_variable([num_outputs])\n",
    "\n",
    "    fc = tf.matmul(x, w + b)\n",
    "\n",
    "    return fc\n",
    "\n",
    "# 3D池化\n",
    "def max_pool3d(x, ksize):\n",
    "    return tf.nn.max_pool3d(x, ksize=[1, ksize[0], ksize[1], ksize[2], 1], strides=[\n",
    "        1, ksize[0], ksize[1], ksize[2], 1], padding='VALID')\n",
    "\n",
    "def avg_pool3d(x, ksize):\n",
    "    return tf.nn.avg_pool3d(x, ksize=[1, ksize[0], ksize[1], ksize[2], 1], strides=[\n",
    "        1, ksize[0], ksize[1], ksize[2], 1], padding='VALID')\n",
    "\n",
    "# 示例\n",
    "input_data = tf.constant(0.1, shape=[1, 4, 4, 4, 1])\n",
    "\n",
    "conv3d1 = conv3d_with_bn(input_data, 3, 1, 8, True)\n",
    "print(conv3d1)\n",
    "\n",
    "maxpool1 = max_pool3d(conv3d1, [2, 2, 2])\n",
    "print(maxpool1)\n",
    "\n",
    "reshaped = tf.reshape(maxpool1, [-1, 8])\n",
    "fc1 = new_fc(reshaped, 8, 2)\n",
    "print(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 3D CNN网络结构的搭建\n",
    "import tensorflow as tf\n",
    "\n",
    "# 定义占位符，设置data的name为data\n",
    "data = tf.placeholder(tf.float32, shape=[None, 32, 64, 64], name='data')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "# 增加维度（channel）\n",
    "input_data = tf.expand_dims(data,axis=-1)     # ? * 32 * 64 * 64 * 1\n",
    "\n",
    "avgpool1 = avg_pool3d(input_data, [2, 1, 1])     # ? * 16 * 64 * 64 * 1\n",
    "\n",
    "conv1 = conv3d_with_bn(avgpool1, 3, 1, 32, is_training)     # ? * 16 * 64 * 64 * 32\n",
    "maxpool1 = max_pool3d(conv1, [1, 2, 2])     # ? * 16 * 32 * 32 * 32\n",
    "\n",
    "conv2 = conv3d_with_bn(maxpool1, 3, 32, 64, is_training)     # ? * 16 * 32 * 32 * 64\n",
    "maxpool2 = max_pool3d(conv2, [2, 2, 2])     # ? * 8 * 16 * 16 * 64\n",
    "\n",
    "conv3 = conv3d_with_bn(maxpool2, 3, 64, 64, is_training)     # ? * 8 * 16 * 16 * 64\n",
    "maxpool3 = max_pool3d(conv3, [2, 2, 2])      # ? * 4 * 8 * 8 * 64\n",
    "\n",
    "conv4 = conv3d_with_bn(maxpool3, 3, 64, 128, is_training)    # ? * 4 * 8 * 8 * 128\n",
    "maxpool4 = max_pool3d(conv4, [2, 2, 2])     # ? * 2 * 4 * 4 * 128\n",
    "\n",
    "conv5 = conv3d_with_bn(maxpool4, 3, 128, 64, is_training)    # ? * 2 * 4 * 4 * 64\n",
    "maxpool5 = max_pool3d(conv5, [2, 2, 2])     # ? * 1 * 2 * 2 * 64\n",
    "\n",
    "conv6 = conv3d_with_bn(maxpool5, 3, 64, 1, is_training)    # ? * 1 * 2 * 2 * 1\n",
    "\n",
    "flatten = tf.reshape(conv6, [-1, 4])      # ? * 4\n",
    "\n",
    "predictions = new_fc(flatten, 4, 2)      # ? * 2\n",
    "\n",
    "# 添加到集合中在重新加载模型时方便获取\n",
    "tf.add_to_collection('predict', predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 损失函数、优化器及模型持久化\n",
    "import tensorflow as tf\n",
    "\n",
    "labels = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "# 设置交叉熵损失函数\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels, predictions)\n",
    "\n",
    "# 用tf.layers.batch_normalization时需要设置\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    \n",
    "    # 设置优化器\n",
    "    optimizer = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 模型的保存和恢复变量\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.5 定义评估函数\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def accuracy(session, x, y, batch_size):\n",
    "    \n",
    "    # losses用来存储每个batch的loss，y_pred用于存储预测类别\n",
    "    losses = []\n",
    "    y_pred = np.array([])\n",
    "    \n",
    "    # 分批次进行评估\n",
    "    for start in range(0, len(x), batch_size):\n",
    "        x_batch = x[start: start + batch_size]\n",
    "        y_batch = y[start: start + batch_size]\n",
    "        \n",
    "        # 将数据喂给占位符\n",
    "        feed_dict = {}\n",
    "        feed_dict[data] = x_batch\n",
    "        feed_dict[labels] = y_batch\n",
    "        \n",
    "        # 评估过程中为预测模式\n",
    "        feed_dict[is_training] = False\n",
    "        \n",
    "        fetches = [loss, tf.nn.softmax(predictions)]\n",
    "\n",
    "        outputs = session.run(fetches, feed_dict)\n",
    "        \n",
    "        # loss操作的返回值是一个数值，需要首先转化为列表\n",
    "        losses += [outputs[0]]\n",
    "        \n",
    "        # 获取模型分类结果\n",
    "        y_pred = np.append(y_pred, np.argmax(outputs[1], axis=1))\n",
    "        \n",
    "   # 获取混淆矩阵及TP、FP、TN、FN\n",
    "    TN, FP, FN, TP = confusion_matrix(y, y_pred).ravel()\n",
    "\n",
    "    # 计算正确率、敏感度、特异度\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    sens = (TP) / (TP + FN)\n",
    "    spec = (TN) / (FP + TN)\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "\n",
    "    return avg_loss, acc, sens, spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.6 数据集的读取与划分\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置文件路径\n",
    "data_dir = './data_dir'\n",
    "labels_file = 'labels.csv'\n",
    "\n",
    "patient_data = os.listdir(data_dir)\n",
    "patient_data.sort()\n",
    "\n",
    "labels_df = pd.read_csv(labels_file)\n",
    "print(labels_df.head())\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "# 读取数据并获取标签存储到列表中\n",
    "for file in patient_data:\n",
    "    \n",
    "    # 获取文件路径 加载数据\n",
    "    path = os.path.join(data_dir, file)\n",
    "    image = np.load(path)\n",
    "    \n",
    "    pat_id = file[:-4]\n",
    "    \n",
    "    # 获取对应的标签\n",
    "    row = labels_df[labels_df['id'] == pat_id]\n",
    "    if len(row) == 1:\n",
    "        label = int(row['cancer'])\n",
    "        x.append(image)\n",
    "        y.append(label)\n",
    "    else:\n",
    "        print(pat_id, \"无标签！\")\n",
    "        \n",
    "x = np.asarray(x, dtype=np.float32)\n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "num_total = x.shape[0]\n",
    "print(\"数据量：%s \" %  num_total)\n",
    "\n",
    "# 数据集划分\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1024, stratify=y)\n",
    "\n",
    "print(\"训练集：{}；测试集：{}\".format(x_train.shape[0], x_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.7 训练模型\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 设置模型保存路径\n",
    "train_dir = './cnn3d_weights/'\n",
    "epochs=5\n",
    "batch_size=8\n",
    "\n",
    "# 在上下文语境中创建TensorFlow会话\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    if os.path.exists(train_dir):\n",
    "               \n",
    "        # 加载模型参数\n",
    "        saver.restore(sess, train_dir + 'cnn3d')\n",
    "    else:\n",
    "        print(\"创建新的模型！\")\n",
    "        \n",
    "        # 初始化所有参数\n",
    "        sess.run(tf.global_variables_initialier())\n",
    "        \n",
    "        # 获取所有可训练变量并打印\n",
    "        parameter_num = ____\n",
    "        print('模型可训练参数总量：%d' % parameter_num)\n",
    "\n",
    "    # 根据回合数进行循环\n",
    "    for e in range(epochs):\n",
    "        print(\"=\" * 80)\n",
    "        print(\"[Epoch %s of %s]\" % (e + 1, epochs))\n",
    "\n",
    "        # 创建训练数据的索引并随机打乱\n",
    "        indices = np.random.permutation(y_train.shape[0])\n",
    "        \n",
    "        # start为每个batch的其实索引，循环的步长为batch_size\n",
    "        for start in range(0, len(x_train), batch_size):\n",
    "            input_feed = {}\n",
    "            input_feed[data] = x_train[indices[start:start + batch_size]]\n",
    "            input_feed[labels] = y_train[indices[start:start + batch_size]]\n",
    "            input_feed[is_training] = True\n",
    "            \n",
    "            output_feed = optimizer\n",
    "\n",
    "            sess.run(output_feed, input_feed)\n",
    "        \n",
    "        # 在一个回合结束后在训练集和测试集上进行验证\n",
    "        train_loss, train_acc, train_sens, train_spec = accuracy(sess, x_train, y_train, batch_size)\n",
    "        print(\"训练集损失：{:.2f}；正确率:{:.2f}；敏感度：{:.2f}；特异度：{:.2f}\".format(train_loss, train_acc, train_sens, train_spec))\n",
    "        \n",
    "        val_loss, val_acc, val_sens, val_spec = ____\n",
    "        print(\"测试集损失：{:.2f}；正确率:{:.2f}；敏感度：{:.2f}；特异度：{:.2f}\".format(val_loss, val_acc, val_sens, val_spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第九章 肺癌诊断模型效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.2 模型评估\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def accuracy_predict(session, x, y, batch_size, show_confusion):\n",
    "    \n",
    "    # 创建评价指标初始化为0， losses用来存储每个batch的loss\n",
    "    losses = []\n",
    "    y_prob = np.array([])\n",
    "    \n",
    "    # 分批次进行评估\n",
    "    for start in range(0, len(x), batch_size):\n",
    "        x_batch = x[start: start + batch_size]\n",
    "        y_batch = y[start: start + batch_size]\n",
    "        \n",
    "        # 将数据喂给占位符\n",
    "        input_feed = {}\n",
    "        input_feed[data] = x_batch\n",
    "        input_feed[labels] = y_batch\n",
    "        \n",
    "        # 评估过程中不需要进行权重更新\n",
    "        input_feed[is_training] = False\n",
    "        \n",
    "        output_feed = [loss, tf.nn.softmax(predictions)]\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        \n",
    "        losses += [outputs[0]]\n",
    "        \n",
    "        # 获取正例的概率\n",
    "        y_prob = np.append(y_prob, outputs[1][:,1])\n",
    "\n",
    "    # 获取混淆矩阵及TP、FP、TN、FN\n",
    "    conf_matrix = confusion_matrix(y, y_prob > 0.5)\n",
    "    \n",
    "    # 转换为DataFrame并打印\n",
    "    if show_confusion:\n",
    "        \n",
    "        ## 设置正常显示中文\n",
    "        sns.set(font='SimHei')\n",
    "        \n",
    "        ## 绘制热力图\n",
    "        ax = sns.heatmap(conf_matrix, annot=True, fmt='d', \n",
    "                 xticklabels=[\"阴性(0)\",\"阳性(1)\"],\n",
    "                 yticklabels=[\"阴性(0)\",\"阳性(1)\"])\n",
    "        \n",
    "        ax.set_ylabel('真实标签')\n",
    "        ax.set_xlabel('预测标签')\n",
    "        ax.set_title('混淆矩阵热力图')\n",
    "    \n",
    "    # 计算正确率、敏感度、特异度\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    acc = (TP + TN) / (TP + FP + TN + FN)\n",
    "    sens = (TP) / (TP + FN)\n",
    "    spec = (TN) / (FP + TN)\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "\n",
    "    return avg_loss, acc, sens, spec, y_prob\n",
    "\n",
    "# 重新创建会话加载模型\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # 加载保存的最小损失模型\n",
    "    saver.restore(sess, train_dir + 'cnn3d')\n",
    "\n",
    "    # 获取模型在测试集上的性能\n",
    "    avg_loss, acc, sens, spec, probs = accuracy_predict(sess, x_test, y_test, 8, True)\n",
    "    print(\"测试集正确率：{:.2f}\".format(acc))\n",
    "    print(\"测试集敏感度：{:.2f}\".format(sens))\n",
    "    print(\"测试集特异度：{:.2f}\".format(spec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.3 绘制ROC曲线图并计算AUC值\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 根据测试集的实际标签和模型输出概率获取roc曲线\n",
    "fpr,tpr, _ = roc_curve(y_test, probs)\n",
    "\n",
    "# 计算auc\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"AUC为：{:.2f}\".format(roc_auc))\n",
    "\n",
    "# 绘图\n",
    "fig = plt.figure()\n",
    "\n",
    "# 根据for和tpr绘制roc图像\n",
    "plt.plot(fpr, tpr, color='darkorange')\n",
    "\n",
    "# 绘制对角线\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "\n",
    "# 设置坐标轴\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "# 设置标题\n",
    "plt.title('ROC曲线')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9.4 肺癌诊断流程\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def predict_class(x):\n",
    "    \n",
    "    train_dir = './cnn3d_weights/'\n",
    "    \n",
    "    predict_data = np.array([x])\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        # 加载计算图，文件名：cnn3d.meta\n",
    "        new_saver = tf.train.import_meta_graph(train_dir + 'cnn3d.meta')\n",
    "        \n",
    "        # 加载模型参数\n",
    "        new_saver.restore(sess, train_dir + 'cnn3d')\n",
    "        \n",
    "        # 获取默认的计算图\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        # 获取用到的张量\n",
    "        data = graph.get_tensor_by_name('data:0')\n",
    "        is_training = graph.get_tensor_by_name('is_training:0')\n",
    "        predictions = tf.get_collection('predict')\n",
    "        \n",
    "        feed_dict = {}\n",
    "        feed_dict[data] = predict_data\n",
    "        feed_dict[is_training] = False\n",
    "        \n",
    "        fetches= tf.nn.softmax(predictions)\n",
    "        \n",
    "        output = sess.run(fetches, feed_dict)\n",
    "        \n",
    "        return np.argmax(output)\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = np.load(\"test.npy\")\n",
    "\n",
    "# 获取预测结果\n",
    "result = predict_class(test_data)\n",
    "\n",
    "print(\"诊断结果：\", \"阳性\" if result == 1 else \"阴性\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
